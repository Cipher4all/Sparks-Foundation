# -*- coding: utf-8 -*-
"""Iris-Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19zHgFZ-7OmLeZ6vBIWAs2Hzzi2kh3lbj
"""

#importing libraries 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#loading iris dataset 
iris_dataset =  pd.read_csv("/content/sample_data/Iris.csv")

#Seeing first 5 rows
iris_dataset.head(5)

#Deleting Id column using Drop function
iris_dataset.drop('Id', axis=1, inplace=True)

#Renaming all the columns 
iris_dataset.rename(columns={'SepalLengthCm': 'Sepal Length (cm)', 'SepalWidthCm': 'Sepal Width (cm)', 'PetalLengthCm': 'Petal Length (cm)', 'PetalWidthCm': 'Petal Width (cm)'}, inplace=True)
iris_dataset.head(5)

#Importing Kmeans classifer
from sklearn.cluster import KMeans

#Convverting dataset into array
x_kmeans = iris_dataset.iloc[:, [0, 1, 2, 3]].values
x_kmeans

# Finding the optimum number of clusters for k-means classification using Elbow Method For KMeans by importing yellowbrick library
from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12))
visualizer.fit(x_kmeans)

# Applying kmeans to the dataset / Creating the kmeans classifier
kmeans = KMeans(n_clusters = 3, init = 'k-means++',
                max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(x_kmeans)

# Visualising the clusters - On the first two columns
plt.scatter(x_kmeans[y_kmeans == 0, 0], x_kmeans[y_kmeans == 0, 1], 
            s = 100, c = 'red', label = 'Iris-setosa')
plt.scatter(x_kmeans[y_kmeans == 1, 0], x_kmeans[y_kmeans == 1, 1], 
            s = 100, c = 'blue', label = 'Iris-versicolour')
plt.scatter(x_kmeans[y_kmeans == 2, 0], x_kmeans[y_kmeans == 2, 1],
            s = 100, c = 'green', label = 'Iris-virginica')

# Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], 
            s = 100, c = 'yellow', label = 'Centroids')

plt.legend()

